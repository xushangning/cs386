\documentclass[journal]{IEEEtran}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
  basicstyle=\fontsize{9}{10}\selectfont\ttfamily,
  numbers=left,
  numberstyle= \tiny,
  keywordstyle= \color{ blue!70},
  commentstyle= \color{red!50!green!50!blue!50},
  frame=single,
  rulesepcolor= \color{ red!20!green!20!blue!20} ,
  escapeinside=``,
  xleftmargin=1.5em,xrightmargin=0em, aboveskip=1em,
  framexleftmargin=2em,
  showstringspaces=false,
  showtabs=false,
  breaklines=true
}
\lstdefinelanguage{Solidity}
{
  morekeywords={contract, mapping, address, uint, private, function, public, if, payable},
  morecomment=[l]{//},
  morestring=[b]"
}


\usepackage{multicol}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{cuted}

\usepackage{amsmath}
\usepackage{extpfeil}
\usepackage{mathpartir}
\usepackage[mathscr]{eucal}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{CJK}

\crefformat{section}{\S#2#1#3} % see manual of cleveref, section 8.2.1
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Oracle Bone Inscription Recognition\\ with CNN and Template Matching}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{
	Zhongye~Wang,~\IEEEmembership{SJTU,}
	Jingyu~Li,~\IEEEmembership{SJTU,}
	Xiaoyi~Bao,~\IEEEmembership{SJTU,}
	Shangning~Xu,~\IEEEmembership{SJTU,}
	and Chenxuan~Li,~\IEEEmembership{SJTU}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
	In this project, we tackle the task of oracle bone inscription (OBI) detection based on the dataset created in class.
	Due to the limit size of dataset, we first augment data with affine transforms and noise transforms to obtain a larger and more robust dataset.
	We then apply a simple CNN and template matching to implement models that can distinguish bone characters between micro categories (40 classes) and macro categories (10 classes).
	We examine models using our homebrew bone character dataset and find the optimal models for micro and macro category classification tasks.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
	OBI recognition, CNN, template matching, data augmentation.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{O}{racle} bone inscription (OBI) is the earliest known Chinese writing, which dates back to the late 2nd millennium BC. Detection and recognition of OBI is crucial for the understanding of the social and political characteristics of ancient Chinese. It is also significant in education and arts nowadays, as OBI is a cultural heritage of both China and the human civilization.

In this paper, we propose different methods to detect hand-written oracle bone script, which facilitates interactive applications. We try both the traditional template matching and the convolutional neural network. We also make an ensemble of the two methods to get a better performance.

\section{Related Work}
For years researchers have tried traditional image processing methods on oracle bone inscription, like template matching \cite{tm}. Meng et al. \cite{tm1} improves this method by devising a two-stage method. They extract line points using Hough transform and generate templates in that space. 

Currently, Deep learning is widely used in the field of object detection and classification with exciting accuracy. Meng et al. \cite{conv} use AlexNet on their own generated dataset of OBI and achieved considerable results. Since in that research, they need to first segment each character before doing the classification, they later proposed a method \cite{ssd} to use Single Shot Multibox Detector (SSD) on OBI.


% wzy
\section{Data Augmentation}
The original dataset contains 40 micro categories of OBIs, each with 85 samples.
Therefore, we have $40 \times 85 = 3400$ samples in total.
This is barely enough for training a convolutional neural network without overfitting, and we need to apply data augmentation to obtain larger dataset adequate for training.

We first make the following observations.

\paragraph{Observation 1} For each pair of characters in Figure~\ref{fig:sym-across}, we may consider them horizontally symmetric.
Thus we can double the number of samples by flipping characters in one category horizontally to obtain those in another category.

\begin{figure}[h]
	\centering
	\begin{minipage}{0.2\linewidth}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-1.png}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-2.png}
	\end{minipage}
	% \hfill
	\begin{minipage}{0.2\linewidth}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-3.png}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-4.png}
	\end{minipage}
	% \hfill
	\begin{minipage}{0.2\linewidth}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-5.png}
		\includegraphics[width=0.8\linewidth]{fig/observation-1-6.png}
	\end{minipage}
	\caption{Symmetric Samples Across Micro-Categories}
	\label{fig:sym-across}
\end{figure}

\paragraph{Observation 2} For characters in Figure~\ref{fig:sym-in}, they are horizontally symmetric by themselves.
We can flip them horizontally to double the number of samples in these categories.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{fig/observation-2.png}
	\caption{Symmetric Samples in Micro-Categories}
	\label{fig:sym-in}
\end{figure}

To balance the samples in different categories after applying above two methods of data augmentation, we need to also double the number of samples in the rest of categories.
We choose to apply affine transforms to the remaining images.
More specifically, we randomly scale images up/down by 0 to 10 percent, randomly translate the image horizontally/vertically by 0 to 10 pixels, randomly rotate and shear images by -10 to 10 degrees.
The augmented images is with the same categories as the original ones.

Then, we need to introduce more noises into the dataset to allow the model trained on it generalize well.
We apply gaussian blur to original images, randomly strengthen/weaken the contrast in each images, randomly add some gaussian noise to each pixel or one channel of images, and randomly make some images brighter or darker by multiplying channels of images.

Figure~\ref{fig:aug-example} shows an example of the affine augmentation and the noise augmentation.
\begin{figure}[h]
	\centering
	\begin{minipage}{0.2\linewidth}
		\subfigure[Original]{
			\includegraphics[width=\linewidth]{fig/person_0001.jpg}
		}
	\end{minipage}
	\begin{minipage}{0.2\linewidth}
		\subfigure[Affine]{
			\includegraphics[width=\linewidth]{fig/person_0001_affine.jpg}
		}
	\end{minipage}
	\begin{minipage}{0.2\linewidth}
		\subfigure[Noise]{
			\includegraphics[width=\linewidth]{fig/person_0001_noise.jpg}
		}
	\end{minipage}
	\caption{An Example of Affine Augmentation and Noise Augmentation}
	\label{fig:aug-example}
\end{figure}

When we actually apply the above augmentation methods to images, we first split the dataset into training set and validation set and only apply augment the samples in the training set to avoid data leakage.
We select 10 images from each micro categories to form the validation set.
Eventually, we end up tripling the size of the dataset with 9120 training samples and 400 validation samples.

\section{Character Recognition}
We use two different methods to address the character recognition task for OBIs.
We first learn a deep convolutional neural network to recognize characters, then use the template matching method for the same task.

% wzy
\subsection{Convolutional Neural Network}
The deep neural network for learning the OBIs has the structure in Figure~\ref{fig:network}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{fig/network.png}
	\caption{Structure of the DNN}
	\label{fig:network}
\end{figure}

The network first uses 2 convolutional layer with 16 filters, then apply max-pooling and batch normalization, then another 2 convolutional layer with 32 filters and pooling layer with normalization. Lastly, it uses one dense layer and an output layer. All trainable layers except the output layer uses ReLU activation and the output layer uses softmax activation.

We use a simple stochastic gradient descent optimizer to train the network on the 9120 training samples and validate it on the remaining 400 samples.
The data has been normalized into the range from 0 to 1.

The network for 10 category classification achieves 98\% accuracy on the validation set at the save point, and the one for 40 category classification achieves 88.5\% accuracy on the validation set at the save point.
Figure~\ref{fig:network-confusion} shows the confusion matrix for the validation results.

\begin{figure}[h]
	\centering
	\begin{minipage}{\linewidth}
		\subfigure[Confusion Matrix for 10 Category Classification]{
			\includegraphics[width=\linewidth]{fig/confusion_oraclenet_conv10.png}
			\label{fig:network-confusion-10}
		}
	\end{minipage}
	\begin{minipage}{\linewidth}
		\hspace{-2em}
		\subfigure[Confusion Matrix for 40 Category Classification]{
			\includegraphics[width=1.15\linewidth]{fig/confusion_oraclenet_conv40.png}
			\label{fig:network-confusion-40}
		}
	\end{minipage}
	\caption{Validation Results}
	\label{fig:network-confusion}
\end{figure}

In Figure~\ref{fig:network-confusion-10}, we can see the classification for 10 macro category is quite accurate, only 7 samples out of 400 is miss-classified.
In Figure~\ref{fig:network-confusion-40}, we use solid lines to separate 10 macro categories among 40 micro categories.
We can see the mistakes made between macro categories is very low and we can use this model for the macro classification task with 98\% accuracy as well.
Most mistakes are made within the macro category.
For example, it miss-classifies many samples for variants of the 3rd and 8th macro categories.

It is possible to improve the performance of DNN method by building an ensemble of the Cat10 network model and Cat40 network model so that we can cancel some errors in one network with the support of another.
We use the outputs from two networks as the inputs to another dense layer to further classify samples based on the first stage results.
We train the extra dense layer still using the train samples.
However, the performance of the ensemble models does not improve wrt. the original DNN.

% ljy
\subsection{Template Matching}
For the OBI recognition task, we also try a traditional method, template matching, to solve it.

The principle is simple. We construct a template for each character and measure the similarity of the sample and templates. The template that has the highest similarity is believed to be the character that sample represents.

In order to construct a template, the intuition is to take all the samples of one category and take the average. However, we observe that the samples of the same character may have different size and position in the box. So the template generated by the above procedure may be blur and unrepresentative. Therefore, we preprocess the samples to find the best focus and scale. First find the center of mass of the box, which is analogous to the same term in physics. We denote the gray scale image as $G$. Consider the blackness as a measure of intensity, so we take the complement image of $G$, denoted as $\hat{G}$. i.e. $\hat{G}(i,j)=255-G(i,j)$. Then we can calculate the center of mass by the following formula:
\begin{align*}
	c_x=\frac{\sum_{i,j}\hat{G}(i,j)\cdot i}{\sum_{i,j} \hat{G}(i,j)} \\
	c_y=\frac{\sum_{i,j}\hat{G}(i,j)\cdot j}{\sum_{i,j} \hat{G}(i,j)}
\end{align*}

To determine the scale, we first set a standard scale for all the templates, say $(M,N)$. Then we search the scaling coefficient $k$ for each sample such that the new box with size $(kM,kN)$ can enclose $\sigma$ of the mass of the original box, where $\sigma\in (0,1)$ is the scaling threshold. When we increase $k$ the new box may exceed the boundary of the original box, so we add padding of all zeros (which represents white in our complement image) to thoses areas. Then we resize the new box to the standard template size. We denote the transformed sample image as $G'$. Average over such new boxes of the samples and the get the templates.

To measure the similarity of a sample and a template, we think of two metrics, normed mean square error and normed correlation, which is defined below:
\begin{align*}
	\text{NMSE}(G',T)= \frac{\sum_{x,y} (T(x,y)-G'(x,y))^2}{\sqrt{\sum_{x,y}T(x,y)^2 \cdot \sum_{x,y} G'(x,y)^2}} \\
	\text{NCOR}(G',T)= \frac{\sum_{x,y} (T(x,y) \cdot G'(x,y))}{\sqrt{\sum_{x,y}T(x,y)^2 \cdot \sum_{x,y} G'(x,y)^2}}
\end{align*}

To compare this method with the above convolutional neural network, we do the same train-validation split.

\section{Experiments}
Then we test the accuracy of our models under more realistic circumstances.

\subsection{Test Set}
We test models on our homebrew dataset where we write OBIs using different pens, on different papers, and captured under different lighting condition.
Figure~\ref{fig:testset} shows samples in the test set.
We can see that the background color and character color varies dramatically from those of the original dataset.
Moreover, there are other noises such as the texture of the paper that could disturb our models.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.19\linewidth]{fig/testset/test_bxy_2_30.jpg}
	\includegraphics[width=0.19\linewidth]{fig/testset/test_lcx_1_40.jpg}
	\includegraphics[width=0.19\linewidth]{fig/testset/test_ljy_2_28.jpg}
	\includegraphics[width=0.19\linewidth]{fig/testset/test_wzy_1_30.jpg}
	\includegraphics[width=0.19\linewidth]{fig/testset/test_xsn_2_28.jpg}
	\caption{Samples in Test Set}
	\label{fig:testset}
\end{figure}

There are 400 samples in the test set, 10 samples for each micro category.
We only write 200 samples and double the size by capturing them under different lighting.

\subsection{Evaluation and Improvements}
To recognize characters in the test set, we need to first preprocess the samples.

We first obtain the grey scale version of the images.
For the test on the neural network, we normalize images as before and set pixels with values less than some threshold $\delta$ to 0.
For the test on the template matching, besides the normalization, we also set pixels with values less than some threshold $\Delta$ to 0.
In this way, we can remove the influence of the background color on the recognition result.

\begin{table}[h]
	\centering
	\caption{Accuracy for Different Models}
	\label{tab:result}
	\begin{tabular}{|c|c|c|}
		\hline
		                            & Cat10  & Cat40  \\\hline
		DNN ($\delta=0.5$)          & \textbf{0.8475} & 0.5275 \\\hline
		DNN Ensemble ($\delta=0.5$) & 0.8000 & 0.56   \\\hline
		TM ($\Delta=150$, NMSE)     & 0.7575 & 0.6125 \\\hline
		TM ($\Delta=150$, NCCOR)    & \textbf{0.8475} & \textbf{0.7000} \\\hline
	\end{tabular}
\end{table}

Table~\ref{tab:result} shows the accuracy of different models wrt. the test set.
It turns out that the DNN is no better than template matching in the case of micro category classification due to its sensitiveness to noises.

\section{Conclusion}
This paper focuses on detection of hand-written oracle bone script. We first manually generate a small dataset and use data augmentation, like affine transform and noise augmentation, to get more data. Then we try light-weight models like template matching and low-depth convolutional neural network. We improve the two methods with scale transformation and ensemble network respectively. The experiment shows that the two methods achieve similar performance, probably because the lack of data impairs CNN's generalization ability.

\bibliographystyle{IEEEtran}
\bibliography{Ref}


% that's all folks
\end{document}


